{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Suppress all pandas warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fights = pd.read_csv('../datasets/raw/ufc-master.csv')\n",
    "df_rankings = pd.read_csv('../datasets/raw/rankings_history.csv')\n",
    "df_fights.head()\n",
    "df_rankings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in numeric columns\n",
    "numeric_cols = df_fights.select_dtypes(include=[np.number]).columns\n",
    "df_fights[numeric_cols] = df_fights[numeric_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models cannot process missing (NaN) values\n",
    "For fight statistics, a missing value often indicates that the stat didn't occur, making 0 a logical replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Winner column\n",
    "df_fights['Winner'] = df_fights['Winner'].map({'Red': 1, 'Blue': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features\n",
    "features = [\n",
    "    # Fighter Records\n",
    "    'RedWins', 'RedLosses', 'RedDraws',\n",
    "    'BlueWins', 'BlueLosses', 'BlueDraws',\n",
    "    \n",
    "    # Win Streaks\n",
    "    'RedCurrentWinStreak', 'BlueCurrentWinStreak',\n",
    "    'RedLongestWinStreak', 'BlueLongestWinStreak',\n",
    "    \n",
    "    # Strike Statistics\n",
    "    'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',\n",
    "    'RedAvgSigStrPct', 'BlueAvgSigStrPct',\n",
    "\n",
    "    # Grappling Statistics\n",
    "    'RedAvgTDLanded', 'BlueAvgTDLanded',\n",
    "    'RedAvgTDPct', 'BlueAvgTDPct',\n",
    "    'RedAvgSubAtt', 'BlueAvgSubAtt'\n",
    "]\n",
    "\n",
    "# Create feature matrix X\n",
    "X = df_fights[features]\n",
    "\n",
    "# Create target vector y\n",
    "y = df_fights['Winner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a curated set of features that are most relevant for fight prediction and defining our feature matrix X and target vector y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForest model with 100 trees for balance between complexity and performance\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    random_state=42    # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model on scaled features\n",
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Calcuate Performanance\n",
    "print(\"\\nClassification Report:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Results Analysis:\n",
    "The model achieves an accuracy of approximately 60% on the test set which is slightly better than random chance (50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win percentages\n",
    "df_fights['RedWinPercentage'] = df_fights['RedWins'] / (df_fights['RedWins'] + df_fights['RedLosses'] + df_fights['RedDraws'])\n",
    "df_fights['BlueWinPercentage'] = df_fights['BlueWins'] / (df_fights['BlueWins'] + df_fights['BlueLosses'] + df_fights['BlueDraws'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature provides:\\\n",
    "Overall career success rate\\\n",
    "More normalized view of fighter performance\\\n",
    "Accounts for different career lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate striking differentials\n",
    "df_fights['StrikingEfficiencyDiff'] = df_fights['RedAvgSigStrPct'] - df_fights['BlueAvgSigStrPct']\n",
    "df_fights['StrikesLandedDiff'] = df_fights['RedAvgSigStrLanded'] - df_fights['BlueAvgSigStrLanded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This captures:\\\n",
    "Technical striking advantage\\\n",
    "Volume striking advantage\\\n",
    "Overall striking dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate grappling differentials\n",
    "df_fights['TakedownEfficiencyDiff'] = df_fights['RedAvgTDPct'] - df_fights['BlueAvgTDPct']\n",
    "df_fights['TakedownsLandedDiff'] = df_fights['RedAvgTDLanded'] - df_fights['BlueAvgTDLanded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This captures:\\\n",
    "Wrestling effectiveness\\\n",
    "Control potential\\\n",
    "Grappling dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate experience metrics\n",
    "df_fights['ExperienceDiff'] = (df_fights['RedTotalRoundsFought'] - df_fights['BlueTotalRoundsFought'])\n",
    "df_fights['TitleBoutExperienceDiff'] = (df_fights['RedTotalTitleBouts'] - df_fights['BlueTotalTitleBouts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This captures:\\\n",
    "Fight experience gap\\\n",
    "High-level competition experience\\\n",
    "Career longevity difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate finish rates\n",
    "df_fights['RedFinishRate'] = (df_fights['RedWinsByKO'] + df_fights['RedWinsBySubmission']) / df_fights['RedWins']\n",
    "df_fights['BlueFinishRate'] = (df_fights['BlueWinsByKO'] + df_fights['BlueWinsBySubmission']) / df_fights['BlueWins']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows:\\\n",
    "Finishing ability\\\n",
    "Fight-ending power\\\n",
    "Submission prowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any infinity or NaN values created during calculations\n",
    "df_fights = df_fights.replace([np.inf, -np.inf], 0)\n",
    "df_fights = df_fights.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up calculated feature columns by checking for problematic values like infinity or extremely large values and replacing inf and -inf with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_features = features + ['RedWinPercentage', 'BlueWinPercentage',\n",
    "    'StrikingEfficiencyDiff', 'StrikesLandedDiff',\n",
    "    'TakedownEfficiencyDiff', 'TakedownsLandedDiff',\n",
    "    'ExperienceDiff', 'TitleBoutExperienceDiff',\n",
    "    'RedFinishRate', 'BlueFinishRate'\n",
    "]\n",
    "\n",
    "# Create feature matrix X\n",
    "X = df_fights[enhanced_features]\n",
    "\n",
    "# Create target vector y\n",
    "y = df_fights['Winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model with same paramaters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  \n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Print model performance\n",
    "print(f\"Model Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': enhanced_features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate metrics\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "sensitivity = tp / (tp + fn)  # True Positive Rate (model's ability to detect Red corner wins)\n",
    "specificity = tn / (tn + fp)  # True Negative Rate (model's ability to detect Blue corner wins)\n",
    "precision = tp / (tp + fp)  #Accuracy of predicted Red corner wins\n",
    "f1 = 2 * (precision * sensitivity) / (precision + sensitivity) # Harmonic mean of precision and recall\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"True Negatives (Blue wins correctly predicted): {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives (Red wins correctly predicted): {tp}\")\n",
    "print(f\"Sensitivity (Proportion of actual Red wins correctly identified): {sensitivity:.3f}\")\n",
    "print(f\"Specificity (Proportion of actual Blue wins correctly identified): {specificity:.3f}\")\n",
    "print(f\"Precision (Proportion of predicted Red wins that were correct): {precision:.3f}\")\n",
    "print(f\"F1 Score (Overall model performance): {f1:.3f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Blue (0)\", \"Red (1)\"], yticklabels=[\"Blue (0)\", \"Red (1)\"])\n",
    "plt.title(\"Confusion Matrix for UFC Winner Prediction Model\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights from Confusion Matrix:\\\n",
    "The F1 score of 0.700 indicates imbalanced performance\\\n",
    "High sensitivity (0.796) shows strong Red corner predictions\\\n",
    "Low specificity (0.333) shows a significant weakness in Blue corner predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
